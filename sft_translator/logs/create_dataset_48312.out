Translate file: /ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/translate_generic.py
python3 translate_generic.py               --model_path=GaMS-Beta/GaMS-9B-SFT-Translator               --input_module=/ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/load_data_scripts/load_nemotron.py               --output_path=test/translations_0.jsonl               --gpu_memory_util=0.95               --tp_size=1               --id=0
cpu-bind=MASK - ixh, task  0  0 [3343964]: mask 0xf000000000000000000000000000f0000000000000000 set
INFO 08-23 11:18:06 [__init__.py:241] Automatically detected platform cuda.
Loading data ...
Selecting examples ...
Number of examples: 27458
Using GaMSTranslatorTaskAdapter adapter for model: GaMS-Beta/GaMS-9B-SFT-Translator
Using Hugging Face model ID: GaMS-Beta/GaMS-9B-SFT-Translator
INFO 08-23 11:18:56 [utils.py:326] non-default args: {'model': 'GaMS-Beta/GaMS-9B-SFT-Translator', 'tokenizer': 'GaMS-Beta/GaMS-9B-SFT-Translator', 'trust_remote_code': True, 'seed': 42, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.95, 'disable_log_stats': True}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 08-23 11:19:02 [__init__.py:711] Resolved architecture: Gemma2ForCausalLM
INFO 08-23 11:19:02 [__init__.py:1750] Using max model len 8192
WARNING 08-23 11:19:02 [arg_utils.py:1770] VLLM_ATTENTION_BACKEND=TORCH_SDPA is not supported by the V1 Engine. Falling back to V0. We recommend to remove VLLM_ATTENTION_BACKEND=TORCH_SDPA from your config in favor of the V1 Engine.
INFO 08-23 11:19:03 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='GaMS-Beta/GaMS-9B-SFT-Translator', speculative_config=None, tokenizer='GaMS-Beta/GaMS-9B-SFT-Translator', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=GaMS-Beta/GaMS-9B-SFT-Translator, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":256,"local_cache_dir":null}, use_cached_outputs=False, 
Traceback (most recent call last):
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/translate_generic.py", line 145, in <module>
    correct_examples(args.model_path, input_module, args.output_path, args.gpu_memory_util, args.tp_size, args.id)
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/translate_generic.py", line 33, in correct_examples
    model = LLM(
  File "/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py", line 285, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py", line 490, in from_engine_args
    return engine_cls.from_vllm_config(
  File "/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py", line 466, in from_vllm_config
    return cls(
  File "/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py", line 257, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
  File "/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py", line 54, in __init__
    self._init_executor()
  File "/usr/local/lib/python3.10/dist-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("init_worker", args=([kwargs], ))
  File "/usr/local/lib/python3.10/dist-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/vllm/utils/__init__.py", line 3007, in run_method
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py", line 592, in init_worker
    self.worker = worker_class(**kwargs)
  File "/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py", line 88, in __init__
    self.model_runner: GPUModelRunnerBase = ModelRunnerClass(
  File "/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py", line 1035, in __init__
    self.attn_backend = get_attn_backend(
  File "/usr/local/lib/python3.10/dist-packages/vllm/attention/selector.py", line 154, in get_attn_backend
    return _cached_get_attn_backend(
  File "/usr/local/lib/python3.10/dist-packages/vllm/attention/selector.py", line 205, in _cached_get_attn_backend
    attention_cls = current_platform.get_attn_backend_cls(
  File "/usr/local/lib/python3.10/dist-packages/vllm/platforms/cuda.py", line 377, in get_attn_backend_cls
    raise ValueError(
ValueError: Invalid attention backend for cuda, with use_v1: False use_mla: False
srun: error: ixh: task 0: Exited with exit code 1
