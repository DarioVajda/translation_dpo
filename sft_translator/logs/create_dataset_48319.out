Translate file: /ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/translate_generic.py
python3 translate_generic.py               --model_path=GaMS-Beta/GaMS-9B-SFT-Translator               --input_module=/ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/load_data_scripts/load_nemotron.py               --output_path=test/translations_0.jsonl               --gpu_memory_util=0.95               --tp_size=1               --id=0
cpu-bind=MASK - ixh, task  0  0 [3351043]: mask 0xf000000000000000000000000000f0000000000000000 set
INFO 08-23 11:19:55 [__init__.py:241] Automatically detected platform cuda.
Loading data ...
Selecting examples ...
Number of examples: 27458
Using GaMSTranslatorTaskAdapter adapter for model: GaMS-Beta/GaMS-9B-SFT-Translator
Using Hugging Face model ID: GaMS-Beta/GaMS-9B-SFT-Translator
INFO 08-23 11:20:46 [utils.py:326] non-default args: {'model': 'GaMS-Beta/GaMS-9B-SFT-Translator', 'tokenizer': 'GaMS-Beta/GaMS-9B-SFT-Translator', 'trust_remote_code': True, 'seed': 42, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.95, 'disable_log_stats': True}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 08-23 11:20:52 [__init__.py:711] Resolved architecture: Gemma2ForCausalLM
INFO 08-23 11:20:52 [__init__.py:1750] Using max model len 8192
INFO 08-23 11:20:52 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:20:54 [core.py:636] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:20:54 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='GaMS-Beta/GaMS-9B-SFT-Translator', speculative_config=None, tokenizer='GaMS-Beta/GaMS-9B-SFT-Translator', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=GaMS-Beta/GaMS-9B-SFT-Translator, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:20:56 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_0 pid=3352806)[0;0m WARNING 08-23 11:20:56 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:20:56 [gpu_model_runner.py:1953] Starting to load model GaMS-Beta/GaMS-9B-SFT-Translator...
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:20:56 [gpu_model_runner.py:1985] Loading model from scratch...
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:20:57 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:20:57 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(EngineCore_0 pid=3352806)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(EngineCore_0 pid=3352806)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.17it/s]
[1;36m(EngineCore_0 pid=3352806)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]
[1;36m(EngineCore_0 pid=3352806)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.20it/s]
[1;36m(EngineCore_0 pid=3352806)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.15it/s]
[1;36m(EngineCore_0 pid=3352806)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]
[1;36m(EngineCore_0 pid=3352806)[0;0m 
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:01 [default_loader.py:262] Loading weights took 3.46 seconds
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:01 [gpu_model_runner.py:2007] Model loading took 17.2181 GiB and 4.314982 seconds
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:10 [backends.py:546] vLLM's torch.compile cache is disabled.
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:10 [backends.py:559] Dynamo bytecode transform time: 7.91 s
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:39 [backends.py:215] Compiling a graph for dynamic shape takes 28.63 s
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:49 [monitor.py:34] torch.compile takes 36.54 s in total
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:50 [gpu_worker.py:276] Available KV cache memory: 48.50 GiB
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:51 [kv_cache_utils.py:1013] GPU KV cache size: 151,344 tokens
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:51 [kv_cache_utils.py:1017] Maximum concurrency for 8,192 tokens per request: 18.46x
[1;36m(EngineCore_0 pid=3352806)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 3/67 [00:00<00:02, 22.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 6/67 [00:00<00:02, 22.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|█▎        | 9/67 [00:00<00:02, 22.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 12/67 [00:00<00:02, 22.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 15/67 [00:00<00:02, 22.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 18/67 [00:00<00:02, 23.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 21/67 [00:00<00:01, 23.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 24/67 [00:01<00:01, 23.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 27/67 [00:01<00:01, 22.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▍     | 30/67 [00:01<00:01, 22.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:01<00:01, 22.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▎    | 36/67 [00:01<00:01, 22.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 39/67 [00:01<00:01, 21.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 42/67 [00:01<00:01, 21.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 45/67 [00:02<00:01, 21.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 48/67 [00:02<00:00, 20.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▌  | 51/67 [00:02<00:00, 20.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 54/67 [00:02<00:00, 19.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▎ | 56/67 [00:02<00:00, 19.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 59/67 [00:02<00:00, 19.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|█████████▎| 62/67 [00:02<00:00, 19.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 65/67 [00:03<00:00, 20.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.53it/s]
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:54 [gpu_model_runner.py:2708] Graph capturing finished in 4 secs, took 0.80 GiB
[1;36m(EngineCore_0 pid=3352806)[0;0m INFO 08-23 11:21:54 [core.py:214] init engine (profile, create kv cache, warmup model) took 52.80 seconds
INFO 08-23 11:21:56 [llm.py:298] Supported_tasks: ['generate']
Preparing prompts ...
Map (num_proc=8):   0%|          | 0/27458 [00:00<?, ? examples/s]Map (num_proc=8):   0%|          | 70/27458 [00:05<38:26, 11.88 examples/s]Map (num_proc=8):   1%|          | 218/27458 [00:05<09:44, 46.61 examples/s]Map (num_proc=8):   1%|▏         | 346/27458 [00:06<05:12, 86.88 examples/s]Map (num_proc=8):   2%|▏         | 480/27458 [00:06<03:09, 142.71 examples/s]Map (num_proc=8):   2%|▏         | 610/27458 [00:06<02:06, 211.80 examples/s]Map (num_proc=8):   3%|▎         | 752/27458 [00:06<01:27, 306.18 examples/s]Map (num_proc=8):   3%|▎         | 900/27458 [00:06<01:02, 423.90 examples/s]Map (num_proc=8):   4%|▍         | 1072/27458 [00:06<00:47, 551.25 examples/s]Map (num_proc=8):   4%|▍         | 1230/27458 [00:06<00:37, 698.73 examples/s]Map (num_proc=8):   5%|▌         | 1423/27458 [00:06<00:28, 908.44 examples/s]Map (num_proc=8):   6%|▌         | 1600/27458 [00:06<00:24, 1073.36 examples/s]Map (num_proc=8):   6%|▋         | 1768/27458 [00:07<00:21, 1202.49 examples/s]Map (num_proc=8):   7%|▋         | 2000/27458 [00:07<00:20, 1263.59 examples/s]Map (num_proc=8):   8%|▊         | 2157/27458 [00:07<00:18, 1332.01 examples/s]Map (num_proc=8):   9%|▊         | 2400/27458 [00:07<00:17, 1419.44 examples/s]Map (num_proc=8):   9%|▉         | 2566/27458 [00:07<00:16, 1472.09 examples/s]Map (num_proc=8):  10%|█         | 2804/27458 [00:07<00:16, 1496.93 examples/s]Map (num_proc=8):  11%|█         | 2986/27458 [00:07<00:15, 1565.16 examples/s]Map (num_proc=8):  12%|█▏        | 3177/27458 [00:08<00:36, 666.20 examples/s] Map (num_proc=8):  12%|█▏        | 3322/27458 [00:08<00:37, 638.51 examples/s]Map (num_proc=8):  13%|█▎        | 3433/27458 [00:09<00:40, 588.66 examples/s]Map (num_proc=8):  13%|█▎        | 3566/27458 [00:09<01:09, 346.26 examples/s]Map (num_proc=8):  14%|█▎        | 3710/27458 [00:09<00:53, 443.02 examples/s]Map (num_proc=8):  14%|█▍        | 3872/27458 [00:10<00:40, 575.39 examples/s]Map (num_proc=8):  15%|█▍        | 4018/27458 [00:10<00:33, 697.61 examples/s]Map (num_proc=8):  15%|█▌        | 4172/27458 [00:10<00:27, 832.31 examples/s]Map (num_proc=8):  16%|█▌        | 4332/27458 [00:10<00:23, 976.26 examples/s]Map (num_proc=8):  16%|█▋        | 4522/27458 [00:10<00:21, 1053.99 examples/s]Map (num_proc=8):  17%|█▋        | 4679/27458 [00:10<00:19, 1161.29 examples/s]Map (num_proc=8):  18%|█▊        | 4872/27458 [00:10<00:18, 1194.82 examples/s]Map (num_proc=8):  18%|█▊        | 5010/27458 [00:10<00:18, 1233.21 examples/s]Map (num_proc=8):  19%|█▉        | 5154/27458 [00:10<00:17, 1276.87 examples/s]Map (num_proc=8):  19%|█▉        | 5294/27458 [00:11<00:17, 1303.31 examples/s]Map (num_proc=8):  20%|█▉        | 5433/27458 [00:11<00:17, 1247.83 examples/s]Map (num_proc=8):  20%|██        | 5576/27458 [00:11<00:16, 1295.40 examples/s]Map (num_proc=8):  21%|██        | 5722/27458 [00:11<00:16, 1331.84 examples/s]Map (num_proc=8):  22%|██▏       | 5915/27458 [00:11<00:16, 1313.17 examples/s]Map (num_proc=8):  22%|██▏       | 6108/27458 [00:11<00:14, 1472.39 examples/s]Map (num_proc=8):  23%|██▎       | 6298/27458 [00:12<00:44, 471.88 examples/s] Map (num_proc=8):  23%|██▎       | 6433/27458 [00:13<01:05, 320.08 examples/s]Map (num_proc=8):  25%|██▌       | 6884/27458 [00:13<00:33, 622.44 examples/s]Map (num_proc=8):  26%|██▌       | 7128/27458 [00:13<00:26, 764.89 examples/s]Map (num_proc=8):  27%|██▋       | 7298/27458 [00:13<00:23, 872.93 examples/s]Map (num_proc=8):  27%|██▋       | 7466/27458 [00:13<00:20, 986.66 examples/s]Map (num_proc=8):  28%|██▊       | 7699/27458 [00:14<00:17, 1116.14 examples/s]Map (num_proc=8):  29%|██▊       | 7866/27458 [00:14<00:16, 1176.82 examples/s]Map (num_proc=8):  29%|██▉       | 8028/27458 [00:14<00:15, 1264.89 examples/s]Map (num_proc=8):  30%|███       | 8244/27458 [00:14<00:14, 1315.85 examples/s]Map (num_proc=8):  31%|███       | 8486/27458 [00:14<00:13, 1396.60 examples/s]Map (num_proc=8):  32%|███▏      | 8715/27458 [00:14<00:13, 1433.51 examples/s]Map (num_proc=8):  32%|███▏      | 8894/27458 [00:14<00:13, 1358.04 examples/s]Map (num_proc=8):  33%|███▎      | 9104/27458 [00:15<00:13, 1366.61 examples/s]Map (num_proc=8):  34%|███▎      | 9252/27458 [00:15<00:13, 1389.07 examples/s]Map (num_proc=8):  34%|███▍      | 9403/27458 [00:15<00:12, 1412.89 examples/s]Map (num_proc=8):  35%|███▍      | 9600/27458 [00:15<00:13, 1369.17 examples/s]Map (num_proc=8):  35%|███▌      | 9745/27458 [00:16<00:34, 515.46 examples/s] Map (num_proc=8):  36%|███▌      | 9866/27458 [00:16<00:43, 407.81 examples/s]Map (num_proc=8):  36%|███▋      | 9986/27458 [00:17<00:55, 312.10 examples/s]Map (num_proc=8):  38%|███▊      | 10473/27458 [00:17<00:24, 685.11 examples/s]Map (num_proc=8):  39%|███▉      | 10678/27458 [00:17<00:21, 786.63 examples/s]Map (num_proc=8):  40%|███▉      | 10896/27458 [00:17<00:18, 902.70 examples/s]Map (num_proc=8):  41%|████      | 11133/27458 [00:17<00:15, 1036.04 examples/s]Map (num_proc=8):  41%|████      | 11300/27458 [00:18<00:14, 1137.26 examples/s]Map (num_proc=8):  42%|████▏     | 11479/27458 [00:18<00:12, 1253.22 examples/s]Map (num_proc=8):  42%|████▏     | 11650/27458 [00:18<00:11, 1346.21 examples/s]Map (num_proc=8):  43%|████▎     | 11900/27458 [00:18<00:10, 1441.71 examples/s]Map (num_proc=8):  44%|████▍     | 12096/27458 [00:18<00:09, 1556.20 examples/s]Map (num_proc=8):  45%|████▍     | 12306/27458 [00:18<00:10, 1497.92 examples/s]Map (num_proc=8):  45%|████▌     | 12474/27458 [00:18<00:09, 1533.32 examples/s]Map (num_proc=8):  46%|████▋     | 12716/27458 [00:18<00:09, 1560.29 examples/s]Map (num_proc=8):  47%|████▋     | 12937/27458 [00:19<00:09, 1526.77 examples/s]Map (num_proc=8):  48%|████▊     | 13115/27458 [00:19<00:10, 1414.65 examples/s]Map (num_proc=8):  48%|████▊     | 13298/27458 [00:19<00:21, 657.42 examples/s] Map (num_proc=8):  49%|████▉     | 13412/27458 [00:20<00:38, 360.61 examples/s]Map (num_proc=8):  49%|████▉     | 13569/27458 [00:21<00:36, 379.87 examples/s]Map (num_proc=8):  50%|█████     | 13778/27458 [00:21<00:26, 524.90 examples/s]Map (num_proc=8):  51%|█████     | 14026/27458 [00:21<00:18, 715.96 examples/s]Map (num_proc=8):  52%|█████▏    | 14194/27458 [00:21<00:15, 843.97 examples/s]Map (num_proc=8):  52%|█████▏    | 14340/27458 [00:21<00:13, 940.44 examples/s]Map (num_proc=8):  53%|█████▎    | 14494/27458 [00:21<00:12, 1050.35 examples/s]Map (num_proc=8):  53%|█████▎    | 14644/27458 [00:21<00:11, 1139.02 examples/s]Map (num_proc=8):  54%|█████▍    | 14820/27458 [00:21<00:11, 1144.05 examples/s]Map (num_proc=8):  55%|█████▍    | 14970/27458 [00:22<00:10, 1219.76 examples/s]Map (num_proc=8):  55%|█████▌    | 15169/27458 [00:22<00:09, 1250.16 examples/s]Map (num_proc=8):  56%|█████▌    | 15395/27458 [00:22<00:09, 1328.93 examples/s]Map (num_proc=8):  57%|█████▋    | 15620/27458 [00:22<00:07, 1538.56 examples/s]Map (num_proc=8):  58%|█████▊    | 15818/27458 [00:22<00:07, 1461.24 examples/s]Map (num_proc=8):  58%|█████▊    | 15991/27458 [00:22<00:07, 1512.50 examples/s]Map (num_proc=8):  59%|█████▉    | 16231/27458 [00:22<00:07, 1537.82 examples/s]Map (num_proc=8):  60%|█████▉    | 16463/27458 [00:23<00:07, 1533.99 examples/s]Map (num_proc=8):  61%|██████    | 16632/27458 [00:23<00:06, 1558.84 examples/s]Map (num_proc=8):  61%|██████▏   | 16830/27458 [00:23<00:13, 817.13 examples/s] Map (num_proc=8):  62%|██████▏   | 16982/27458 [00:23<00:14, 716.52 examples/s]Map (num_proc=8):  62%|██████▏   | 17129/27458 [00:24<00:17, 584.81 examples/s]Map (num_proc=8):  63%|██████▎   | 17222/27458 [00:25<00:34, 299.83 examples/s]Map (num_proc=8):  63%|██████▎   | 17434/27458 [00:25<00:23, 432.58 examples/s]Map (num_proc=8):  64%|██████▍   | 17592/27458 [00:25<00:18, 545.84 examples/s]Map (num_proc=8):  65%|██████▍   | 17744/27458 [00:25<00:14, 665.85 examples/s]Map (num_proc=8):  65%|██████▌   | 17900/27458 [00:25<00:11, 797.36 examples/s]Map (num_proc=8):  66%|██████▌   | 18060/27458 [00:25<00:10, 937.62 examples/s]Map (num_proc=8):  66%|██████▋   | 18256/27458 [00:25<00:08, 1037.96 examples/s]Map (num_proc=8):  67%|██████▋   | 18414/27458 [00:26<00:07, 1146.05 examples/s]Map (num_proc=8):  68%|██████▊   | 18568/27458 [00:26<00:07, 1235.10 examples/s]Map (num_proc=8):  68%|██████▊   | 18720/27458 [00:26<00:06, 1299.28 examples/s]Map (num_proc=8):  69%|██████▉   | 18913/27458 [00:26<00:06, 1289.55 examples/s]Map (num_proc=8):  70%|██████▉   | 19104/27458 [00:26<00:06, 1272.22 examples/s]Map (num_proc=8):  70%|███████   | 19301/27458 [00:26<00:06, 1254.13 examples/s]Map (num_proc=8):  71%|███████   | 19446/27458 [00:26<00:06, 1291.60 examples/s]Map (num_proc=8):  71%|███████▏  | 19594/27458 [00:26<00:05, 1328.23 examples/s]Map (num_proc=8):  72%|███████▏  | 19794/27458 [00:27<00:05, 1322.86 examples/s]Map (num_proc=8):  73%|███████▎  | 19930/27458 [00:27<00:07, 1059.03 examples/s]Map (num_proc=8):  73%|███████▎  | 20078/27458 [00:27<00:08, 866.34 examples/s] Map (num_proc=8):  74%|███████▎  | 20206/27458 [00:27<00:11, 604.53 examples/s]Map (num_proc=8):  74%|███████▍  | 20358/27458 [00:28<00:15, 462.84 examples/s]Map (num_proc=8):  75%|███████▍  | 20516/27458 [00:28<00:12, 574.07 examples/s]Map (num_proc=8):  75%|███████▌  | 20649/27458 [00:29<00:15, 451.96 examples/s]Map (num_proc=8):  76%|███████▌  | 20803/27458 [00:29<00:11, 563.01 examples/s]Map (num_proc=8):  76%|███████▌  | 20920/27458 [00:29<00:10, 646.96 examples/s]Map (num_proc=8):  77%|███████▋  | 21031/27458 [00:29<00:08, 723.45 examples/s]Map (num_proc=8):  77%|███████▋  | 21192/27458 [00:29<00:07, 892.46 examples/s]Map (num_proc=8):  78%|███████▊  | 21356/27458 [00:29<00:05, 1049.09 examples/s]Map (num_proc=8):  78%|███████▊  | 21523/27458 [00:29<00:04, 1194.08 examples/s]Map (num_proc=8):  79%|███████▉  | 21714/27458 [00:29<00:04, 1181.52 examples/s]Map (num_proc=8):  80%|███████▉  | 21865/27458 [00:29<00:04, 1257.08 examples/s]Map (num_proc=8):  80%|████████  | 22056/27458 [00:30<00:04, 1253.10 examples/s]Map (num_proc=8):  81%|████████  | 22240/27458 [00:30<00:04, 1233.85 examples/s]Map (num_proc=8):  82%|████████▏ | 22408/27458 [00:30<00:03, 1336.54 examples/s]Map (num_proc=8):  82%|████████▏ | 22572/27458 [00:30<00:03, 1378.67 examples/s]Map (num_proc=8):  83%|████████▎ | 22722/27458 [00:30<00:03, 1244.49 examples/s]Map (num_proc=8):  83%|████████▎ | 22858/27458 [00:30<00:03, 1264.40 examples/s]Map (num_proc=8):  84%|████████▍ | 23034/27458 [00:30<00:03, 1227.35 examples/s]Map (num_proc=8):  84%|████████▍ | 23176/27458 [00:30<00:03, 1271.95 examples/s]Map (num_proc=8):  85%|████████▍ | 23326/27458 [00:31<00:03, 1326.98 examples/s]Map (num_proc=8):  86%|████████▌ | 23488/27458 [00:31<00:02, 1399.06 examples/s]Map (num_proc=8):  86%|████████▌ | 23680/27458 [00:31<00:02, 1349.99 examples/s]Map (num_proc=8):  87%|████████▋ | 23834/27458 [00:31<00:02, 1392.00 examples/s]Map (num_proc=8):  88%|████████▊ | 24026/27458 [00:31<00:02, 1378.72 examples/s]Map (num_proc=8):  88%|████████▊ | 24234/27458 [00:32<00:07, 407.66 examples/s] Map (num_proc=8):  89%|████████▊ | 24364/27458 [00:32<00:06, 485.33 examples/s]Map (num_proc=8):  89%|████████▉ | 24504/27458 [00:32<00:05, 585.91 examples/s]Map (num_proc=8):  90%|████████▉ | 24647/27458 [00:33<00:04, 700.09 examples/s]Map (num_proc=8):  90%|█████████ | 24784/27458 [00:33<00:03, 808.29 examples/s]Map (num_proc=8):  91%|█████████ | 24942/27458 [00:33<00:02, 953.28 examples/s]Map (num_proc=8):  91%|█████████▏| 25124/27458 [00:33<00:02, 1022.07 examples/s]Map (num_proc=8):  92%|█████████▏| 25281/27458 [00:33<00:01, 1138.64 examples/s]Map (num_proc=8):  93%|█████████▎| 25448/27458 [00:33<00:01, 1256.88 examples/s]Map (num_proc=8):  93%|█████████▎| 25642/27458 [00:33<00:01, 1426.28 examples/s]Map (num_proc=8):  94%|█████████▍| 25858/27458 [00:33<00:01, 1427.78 examples/s]Map (num_proc=8):  95%|█████████▍| 26052/27458 [00:34<00:01, 1374.13 examples/s]Map (num_proc=8):  95%|█████████▌| 26216/27458 [00:34<00:00, 1435.38 examples/s]Map (num_proc=8):  96%|█████████▌| 26424/27458 [00:34<00:00, 1414.37 examples/s]Map (num_proc=8):  97%|█████████▋| 26658/27458 [00:34<00:00, 1458.27 examples/s]Map (num_proc=8):  98%|█████████▊| 26816/27458 [00:34<00:00, 1484.91 examples/s]Map (num_proc=8):  98%|█████████▊| 26976/27458 [00:34<00:00, 1501.78 examples/s]Map (num_proc=8):  99%|█████████▉| 27172/27458 [00:34<00:00, 1424.79 examples/s]Map (num_proc=8): 100%|█████████▉| 27322/27458 [00:34<00:00, 1441.99 examples/s]Map (num_proc=8): 100%|██████████| 27458/27458 [00:35<00:00, 776.64 examples/s] 
Filter (num_proc=8):   0%|          | 0/27458 [00:00<?, ? examples/s]Filter (num_proc=8):  13%|█▎        | 3433/27458 [00:00<00:03, 7325.72 examples/s]Filter (num_proc=8):  25%|██▌       | 6866/27458 [00:00<00:01, 12276.81 examples/s]Filter (num_proc=8):  38%|███▊      | 10298/27458 [00:00<00:01, 16062.68 examples/s]Filter (num_proc=8):  50%|█████     | 13730/27458 [00:00<00:00, 19681.64 examples/s]Filter (num_proc=8):  63%|██████▎   | 17162/27458 [00:00<00:00, 21962.16 examples/s]Filter (num_proc=8):  75%|███████▌  | 20594/27458 [00:01<00:00, 22907.65 examples/s]Filter (num_proc=8):  88%|████████▊ | 24026/27458 [00:01<00:00, 25080.53 examples/s]Filter (num_proc=8): 100%|██████████| 27458/27458 [00:01<00:00, 26570.81 examples/s]Filter (num_proc=8): 100%|██████████| 27458/27458 [00:01<00:00, 18121.68 examples/s]
Number of prompts: 27458
Running translations ...
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [dump_input.py:69] Dumping input data for V1 LLM engine (v0.10.1.1) with config: model='GaMS-Beta/GaMS-9B-SFT-Translator', speculative_config=None, tokenizer='GaMS-Beta/GaMS-9B-SFT-Translator', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=GaMS-Beta/GaMS-9B-SFT-Translator, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"/root/.cache/vllm/torch_compile_cache/9cb5659f1f","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":"/root/.cache/vllm/torch_compile_cache/9cb5659f1f/rank_0_0/backbone"}, 
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [dump_input.py:76] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=0,prompt_token_ids_len=84,mm_kwargs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.6, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8192, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]),num_computed_tokens=0,lora_request=None)], scheduled_cached_reqs=CachedRequestData(req_ids=[], resumed_from_preemption=[], new_token_ids=[], new_block_ids=[], num_computed_tokens=[]), num_scheduled_tokens={0: 84}, total_num_scheduled_tokens=84, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[0, 0], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702] EngineCore encountered a fatal error.
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702] Traceback (most recent call last):
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 693, in run_engine_core
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     engine_core.run_busy_loop()
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 720, in run_busy_loop
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     self._process_engine_step()
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 745, in _process_engine_step
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     outputs, model_executed = self.step_fn()
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 288, in step
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     model_output = self.execute_model_with_error_logging(
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 274, in execute_model_with_error_logging
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     raise err
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 265, in execute_model_with_error_logging
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return model_fn(scheduler_output)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/abstract.py", line 87, in execute_model
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     output = self.collective_rpc("execute_model",
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/utils/__init__.py", line 3007, in run_method
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py", line 362, in execute_model
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 1611, in execute_model
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     model_output = self.model(
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/gemma2.py", line 404, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     hidden_states = self.model(input_ids, positions, intermediate_tensors,
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py", line 279, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     model_output = self.forward(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/gemma2.py", line 277, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     def forward(
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return fn(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 830, in call_wrapped
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 406, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     raise e
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 393, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "<eval_with_key>.86", line 353, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     submod_1 = self.submod_1(getitem, s0, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 830, in call_wrapped
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 406, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     raise e
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 393, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "<eval_with_key>.2", line 5, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_1, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_1 = unified_attention_with_output = None
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1158, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return self._op(*args, **(kwargs or {}))
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py", line 499, in unified_attention_with_output
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     self.impl.forward(self,
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/attention/backends/flash_attn.py", line 533, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     flash_attn_varlen_func(
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/vllm/vllm_flash_attn/flash_attn_interface.py", line 258, in flash_attn_varlen_func
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1158, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702]     return self._op(*args, **(kwargs or {}))
[1;36m(EngineCore_0 pid=3352806)[0;0m ERROR 08-23 11:22:33 [core.py:702] RuntimeError: This flash attention build does not support tanh softcapping.
Adding requests:   0%|          | 0/27458 [00:00<?, ?it/s][1;36m(EngineCore_0 pid=3352806)[0;0m Process EngineCore_0:
[1;36m(EngineCore_0 pid=3352806)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_0 pid=3352806)[0;0m     self.run()
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_0 pid=3352806)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 704, in run_engine_core
[1;36m(EngineCore_0 pid=3352806)[0;0m     raise e
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 693, in run_engine_core
[1;36m(EngineCore_0 pid=3352806)[0;0m     engine_core.run_busy_loop()
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 720, in run_busy_loop
[1;36m(EngineCore_0 pid=3352806)[0;0m     self._process_engine_step()
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 745, in _process_engine_step
[1;36m(EngineCore_0 pid=3352806)[0;0m     outputs, model_executed = self.step_fn()
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 288, in step
[1;36m(EngineCore_0 pid=3352806)[0;0m     model_output = self.execute_model_with_error_logging(
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 274, in execute_model_with_error_logging
[1;36m(EngineCore_0 pid=3352806)[0;0m     raise err
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py", line 265, in execute_model_with_error_logging
[1;36m(EngineCore_0 pid=3352806)[0;0m     return model_fn(scheduler_output)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/executor/abstract.py", line 87, in execute_model
[1;36m(EngineCore_0 pid=3352806)[0;0m     output = self.collective_rpc("execute_model",
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_0 pid=3352806)[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/utils/__init__.py", line 3007, in run_method
[1;36m(EngineCore_0 pid=3352806)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_0 pid=3352806)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py", line 362, in execute_model
[1;36m(EngineCore_0 pid=3352806)[0;0m     output = self.model_runner.execute_model(scheduler_output,
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_0 pid=3352806)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 1611, in execute_model
[1;36m(EngineCore_0 pid=3352806)[0;0m     model_output = self.model(
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/gemma2.py", line 404, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m     hidden_states = self.model(input_ids, positions, intermediate_tensors,
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py", line 279, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m     model_output = self.forward(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/gemma2.py", line 277, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m     def forward(
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
[1;36m(EngineCore_0 pid=3352806)[0;0m     return fn(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 830, in call_wrapped
[1;36m(EngineCore_0 pid=3352806)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 406, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m     raise e
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 393, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "<eval_with_key>.86", line 353, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m     submod_1 = self.submod_1(getitem, s0, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 830, in call_wrapped
[1;36m(EngineCore_0 pid=3352806)[0;0m     return self._wrapped_call(self, *args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 406, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m     raise e
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/fx/graph_module.py", line 393, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=3352806)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "<eval_with_key>.2", line 5, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_1, 'model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_1 = unified_attention_with_output = None
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1158, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m     return self._op(*args, **(kwargs or {}))
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py", line 499, in unified_attention_with_output
[1;36m(EngineCore_0 pid=3352806)[0;0m     self.impl.forward(self,
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/v1/attention/backends/flash_attn.py", line 533, in forward
[1;36m(EngineCore_0 pid=3352806)[0;0m     flash_attn_varlen_func(
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/vllm/vllm_flash_attn/flash_attn_interface.py", line 258, in flash_attn_varlen_func
[1;36m(EngineCore_0 pid=3352806)[0;0m     out, softmax_lse, _, _ = torch.ops._vllm_fa3_C.fwd(
[1;36m(EngineCore_0 pid=3352806)[0;0m   File "/usr/local/lib/python3.10/dist-packages/torch/_ops.py", line 1158, in __call__
[1;36m(EngineCore_0 pid=3352806)[0;0m     return self._op(*args, **(kwargs or {}))
[1;36m(EngineCore_0 pid=3352806)[0;0m RuntimeError: This flash attention build does not support tanh softcapping.
Adding requests:   0%|          | 9/27458 [00:00<00:39, 692.64it/s]
Traceback (most recent call last):
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/translate_generic.py", line 145, in <module>
    correct_examples(args.model_path, input_module, args.output_path, args.gpu_memory_util, args.tp_size, args.id)
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/translate_generic.py", line 68, in correct_examples
    responses = model.generate(prompts, sampling_params=sampling_params)
  File "/usr/local/lib/python3.10/dist-packages/vllm/utils/__init__.py", line 1557, in inner
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py", line 488, in generate
    self._validate_and_add_requests(
  File "/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py", line 1666, in _validate_and_add_requests
    self._add_request(
  File "/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py", line 1684, in _add_request
    self.llm_engine.add_request(
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/llm_engine.py", line 216, in add_request
    self.engine_core.add_request(request)
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py", line 703, in add_request
    self._send_input(EngineCoreRequestType.ADD, request)
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py", line 674, in _send_input
    self.ensure_alive()
  File "/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py", line 526, in ensure_alive
    raise EngineDeadError()
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
srun: error: ixh: task 0: Exited with exit code 1
