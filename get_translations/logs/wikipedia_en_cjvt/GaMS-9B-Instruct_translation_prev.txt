cpu-bind=MASK - gn42, task  0  0 [846328]: mask 0xffffffff000000000000000000000000ffffffff set
Number of examples: 100
WARNING 04-07 13:23:10 config.py:185] gemma2 has interleaved attention, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
INFO 04-07 13:23:14 config.py:905] Defaulting to use mp for distributed inference
INFO 04-07 13:23:14 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='cjvt/GaMS-9B-Instruct', speculative_config=None, tokenizer='cjvt/GaMS-9B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=cjvt/GaMS-9B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
WARNING 04-07 13:23:15 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-07 13:23:15 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:17 utils.py:1008] Found nccl from library libnccl.so.2
INFO 04-07 13:23:17 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:17 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:17 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:17 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:17 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 04-07 13:23:17 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:17 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 04-07 13:23:21 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /ceph/hpc/home/vresd/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:21 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /ceph/hpc/home/vresd/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:21 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /ceph/hpc/home/vresd/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:21 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /ceph/hpc/home/vresd/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 04-07 13:23:21 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fb3d67810a0>, local_subscribe_port=53385, remote_subscribe_port=None)
INFO 04-07 13:23:21 model_runner.py:1056] Starting to load model cjvt/GaMS-9B-Instruct...
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:21 model_runner.py:1056] Starting to load model cjvt/GaMS-9B-Instruct...
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:21 model_runner.py:1056] Starting to load model cjvt/GaMS-9B-Instruct...
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:21 model_runner.py:1056] Starting to load model cjvt/GaMS-9B-Instruct...
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:24 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 04-07 13:23:24 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:24 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:24 weight_utils.py:243] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.87it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.72it/s]
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:27 model_runner.py:1067] Loading model weights took 4.3498 GB
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.00it/s]
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:27 model_runner.py:1067] Loading model weights took 4.3498 GB
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  2.08it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  2.00it/s]

INFO 04-07 13:23:27 model_runner.py:1067] Loading model weights took 4.3498 GB
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:28 model_runner.py:1067] Loading model weights took 4.3498 GB
INFO 04-07 13:23:53 distributed_gpu_executor.py:57] # GPU blocks: 11165, # CPU blocks: 3120
INFO 04-07 13:23:53 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 43.61x
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:55 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:23:55 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:55 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:23:55 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:55 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:23:55 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 04-07 13:23:55 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 04-07 13:23:55 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:24:22 custom_all_reduce.py:233] Registering 2975 cuda graph addresses
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:24:22 custom_all_reduce.py:233] Registering 2975 cuda graph addresses
INFO 04-07 13:24:22 custom_all_reduce.py:233] Registering 2975 cuda graph addresses
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:24:22 custom_all_reduce.py:233] Registering 2975 cuda graph addresses
INFO 04-07 13:24:22 model_runner.py:1523] Graph capturing finished in 27 secs.
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:24:22 model_runner.py:1523] Graph capturing finished in 27 secs.
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:24:22 model_runner.py:1523] Graph capturing finished in 27 secs.
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:24:22 model_runner.py:1523] Graph capturing finished in 27 secs.
Preparing prompts ...
Number of prompts: 100
Running translations ...
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:02<03:46,  2.29s/it, est. speed input: 20.94 toks/s, output: 11.78 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:41,  1.04s/it, est. speed input: 40.35 toks/s, output: 24.86 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:02<01:18,  1.24it/s, est. speed input: 58.30 toks/s, output: 39.54 toks/s]Processed prompts:   4%|â–         | 4/100 [00:03<00:56,  1.70it/s, est. speed input: 79.03 toks/s, output: 57.42 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:03<00:33,  2.78it/s, est. speed input: 119.90 toks/s, output: 94.85 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:28,  3.25it/s, est. speed input: 140.25 toks/s, output: 114.19 toks/s]Processed prompts:   8%|â–Š         | 8/100 [00:03<00:25,  3.63it/s, est. speed input: 160.67 toks/s, output: 133.08 toks/s]Processed prompts:  10%|â–ˆ         | 10/100 [00:04<00:18,  4.85it/s, est. speed input: 201.64 toks/s, output: 173.97 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:04<00:19,  4.59it/s, est. speed input: 215.21 toks/s, output: 188.67 toks/s]Processed prompts:  14%|â–ˆâ–        | 14/100 [00:04<00:11,  7.17it/s, est. speed input: 276.02 toks/s, output: 255.37 toks/s]Processed prompts:  17%|â–ˆâ–‹        | 17/100 [00:04<00:09,  8.53it/s, est. speed input: 453.15 toks/s, output: 316.77 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:05<00:12,  6.54it/s, est. speed input: 450.81 toks/s, output: 323.90 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:05<00:11,  6.69it/s, est. speed input: 480.78 toks/s, output: 361.54 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:05<00:10,  7.66it/s, est. speed input: 532.57 toks/s, output: 425.26 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:08,  8.53it/s, est. speed input: 570.71 toks/s, output: 472.09 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:06<00:11,  6.13it/s, est. speed input: 569.48 toks/s, output: 490.65 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:07<00:13,  5.17it/s, est. speed input: 576.34 toks/s, output: 515.42 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:07<00:12,  5.62it/s, est. speed input: 683.34 toks/s, output: 540.88 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:09,  6.67it/s, est. speed input: 754.77 toks/s, output: 634.08 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:08<00:12,  5.41it/s, est. speed input: 750.67 toks/s, output: 638.60 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:08<00:12,  5.05it/s, est. speed input: 754.55 toks/s, output: 653.63 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:08<00:10,  5.73it/s, est. speed input: 790.76 toks/s, output: 704.58 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:10,  5.78it/s, est. speed input: 817.76 toks/s, output: 748.75 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:08<00:09,  6.23it/s, est. speed input: 842.55 toks/s, output: 776.59 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:09<00:08,  7.03it/s, est. speed input: 896.15 toks/s, output: 831.96 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:09<00:07,  7.04it/s, est. speed input: 916.12 toks/s, output: 856.86 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:09<00:07,  7.07it/s, est. speed input: 942.68 toks/s, output: 882.07 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:09<00:06,  8.58it/s, est. speed input: 993.83 toks/s, output: 943.65 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:09<00:05,  9.48it/s, est. speed input: 1052.88 toks/s, output: 1003.58 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:09<00:05,  8.97it/s, est. speed input: 1080.17 toks/s, output: 1028.73 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:10<00:07,  7.00it/s, est. speed input: 1081.22 toks/s, output: 1042.62 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:10<00:05,  8.28it/s, est. speed input: 1131.86 toks/s, output: 1103.76 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:10<00:08,  5.52it/s, est. speed input: 1122.52 toks/s, output: 1103.27 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:10<00:06,  7.07it/s, est. speed input: 1172.98 toks/s, output: 1168.12 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:11<00:07,  5.68it/s, est. speed input: 1197.90 toks/s, output: 1199.30 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:11<00:07,  5.72it/s, est. speed input: 1214.01 toks/s, output: 1223.18 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:11<00:08,  4.99it/s, est. speed input: 1217.17 toks/s, output: 1235.32 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:12<00:11,  3.36it/s, est. speed input: 1198.58 toks/s, output: 1230.89 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:12<00:09,  3.75it/s, est. speed input: 1218.25 toks/s, output: 1259.87 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:13<00:08,  4.31it/s, est. speed input: 1255.20 toks/s, output: 1313.36 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:13<00:05,  5.83it/s, est. speed input: 1318.40 toks/s, output: 1390.16 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:13<00:05,  5.53it/s, est. speed input: 1351.31 toks/s, output: 1438.56 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:13<00:05,  5.90it/s, est. speed input: 1371.49 toks/s, output: 1471.21 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:14<00:05,  5.04it/s, est. speed input: 1397.10 toks/s, output: 1509.96 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:14<00:05,  4.99it/s, est. speed input: 1410.92 toks/s, output: 1534.61 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:14<00:06,  4.30it/s, est. speed input: 1416.70 toks/s, output: 1546.25 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:15<00:07,  3.30it/s, est. speed input: 1412.73 toks/s, output: 1541.67 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:15<00:05,  4.08it/s, est. speed input: 1464.62 toks/s, output: 1604.58 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:16<00:05,  4.16it/s, est. speed input: 1481.75 toks/s, output: 1630.49 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:16<00:08,  2.42it/s, est. speed input: 1433.52 toks/s, output: 1588.72 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:17<00:06,  2.97it/s, est. speed input: 1464.29 toks/s, output: 1627.82 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:17<00:05,  3.30it/s, est. speed input: 1486.41 toks/s, output: 1657.76 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:18<00:09,  1.88it/s, est. speed input: 1434.91 toks/s, output: 1610.56 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:20<00:10,  1.45it/s, est. speed input: 1384.45 toks/s, output: 1568.69 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:21<00:12,  1.24it/s, est. speed input: 1344.93 toks/s, output: 1538.41 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:21<00:08,  1.56it/s, est. speed input: 1384.43 toks/s, output: 1580.15 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:21<00:07,  1.66it/s, est. speed input: 1397.70 toks/s, output: 1599.38 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:22<00:06,  1.74it/s, est. speed input: 1447.40 toks/s, output: 1619.24 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:22<00:05,  1.84it/s, est. speed input: 1500.29 toks/s, output: 1642.18 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:24<00:08,  1.22it/s, est. speed input: 1442.73 toks/s, output: 1600.14 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:25<00:07,  1.22it/s, est. speed input: 1436.97 toks/s, output: 1606.53 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:25<00:06,  1.28it/s, est. speed input: 1460.29 toks/s, output: 1623.44 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:26<00:05,  1.34it/s, est. speed input: 1517.25 toks/s, output: 1642.68 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:29<00:04,  1.03it/s, est. speed input: 1505.64 toks/s, output: 1622.02 toks/s]Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:31<00:05,  1.33s/it, est. speed input: 1436.00 toks/s, output: 1562.22 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:32<00:03,  1.32s/it, est. speed input: 1438.16 toks/s, output: 1568.34 toks/s]Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:33<00:02,  1.16s/it, est. speed input: 1461.38 toks/s, output: 1602.01 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:33<00:00,  1.15it/s, est. speed input: 1509.26 toks/s, output: 1663.44 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:38<00:00,  2.09s/it, est. speed input: 1341.00 toks/s, output: 1517.06 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:38<00:00,  2.58it/s, est. speed input: 1341.00 toks/s, output: 1517.06 toks/s]
Processing translations ...
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 5526.89 examples/s]
Saving translations ...
  0%|          | 0/100 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 7955.66it/s]
Done!
INFO 04-07 13:25:02 multiproc_worker_utils.py:133] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=846666)[0;0m INFO 04-07 13:25:02 multiproc_worker_utils.py:240] Worker exiting
[1;36m(VllmWorkerProcess pid=846667)[0;0m INFO 04-07 13:25:02 multiproc_worker_utils.py:240] Worker exiting
[1;36m(VllmWorkerProcess pid=846668)[0;0m INFO 04-07 13:25:02 multiproc_worker_utils.py:240] Worker exiting
[rank0]:[W407 13:25:04.392445357 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
